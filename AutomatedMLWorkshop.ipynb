{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# [Automated Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-automated-ml)\nIn this example, we show how automated ML can be used for energy demand forecasting. This notebook is for demonstration purpose only.\n<b> Automated ML Feedback : AskAutomatedML@Microsoft.com <b/>\n\n## Contents\n1. [How to use Jupyter notebooks](# How to use Jupyter Notebook)\n2. [Scenario](#Scenario)\n3. [Data](#Data)\n4. [Train](#Train)\n5. [Deploy](#Deploy)\n\n\n    \n### Note : Pay attention to <font color =\"red\"> Red Text </font> for \"Action Required\" in Cells "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# How to use Jupyter Notebook\nIn this workshop we will be using Jupyter notebooks hosted by Azure Notebooks Service. If you are familiar with Jupyter, skip this. If not, here are a few tips\n\n>* Select the Kernet Python 3.6\n>* Jupyter has 'cells'. When you click on anywhere in the notebook, the cell containing the pointer gets selected.\n>* To run or execute the code in the cell - on the top menu select Cell->Run Cells. Alternatively you can use 'Shift+Enter' keys.\n>* To Edit cell - Double click the cell\n>* When you run a cell, the pointer pointing just outside on the left side of the cell will change from [] to [*] (inserts * in the square brackets). When the cell finished executing the '*' changes to a number.\n>* If the notebook or cell hangs, select from the top menu Kernel->Restart & Clear output for the entire notebook or Kernel->Interrupt for just that cell. Restart will take a few seconds and you would need to reexecute all the cells.\n>* In Jupyter you do not need to execute in sequence. You can go back to a previous cell that was already executed and execute again. As long as there are no dependencies that messes up the code in that particular cell, you will be fine."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Scenario:\n\nThis scenario focuses on energy demand forecasting where the __goal is to predict the future load on an energy grid__. It is a critical business operation for companies in the energy sector as operators need to maintain the fine balance between the energy consumed on a grid and the energy supplied to it. Too much power supplied to the grid can result in waste of energy or technical faults. However, if too little power is supplied it can lead to blackouts, leaving customers without power. Typically, grid operators can take short-term decisions to manage energy supply to the grid and keep the load in balance. An accurate short-term forecast of energy demand is therefore essential for the operator to make these decisions with confidence.\n\nThis scenario details the construction of a machine learning energy demand forecasting solution. The solution is trained on a public dataset from the New York Independent System Operator (NYISO), which operates the power grid for New York State. The dataset includes hourly power demand data for New York City over a period of five years. An additional dataset containing hourly weather conditions in New York City over the same time period was taken from darksky.net.\n\nIn this notebook you will perform\n\n1. Basic Setup: You would need to create an Azure Machine Learning workspace to start with. You can do that in Azure Portal or through SDK. The workspace can be shared with your team mates to collaborate on the scripts. You would create an workspace one time and run multiple experiments and build many models.\n\n2. Prepare the data: Prepare the data for training.\n\n3. Train an automated ML model: We will use Azure Notebooks to train models with automated ML generated pipeline parameters.\n\n4. Explore the results Inspect various way to get all the models built, Many different model metrics\n\n5. Testing the fitted model with some test data\n\n6. Register the model for deployment and get ready for deployment into Azure Machine Learning model management and deployment hosting"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Basic Setiup\nPre requisites required for configuring your notebook to run Automated ML, setting up your Azure Learning Workspace for the experiment and creating your config.json."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Install Additional Packages\nThis is validation step to ensure you have the latest packages to run the exeripment"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# auto ml core packages update\n!pip install --upgrade azureml-sdk[automl]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# auto ml core packages update\n!pip install --upgrade azureml-sdk[notebooks]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# auto ml core packages update\n!pip install --upgrade azureml-sdk[explain]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Webservice schema for Swagger compatibility. Install will take a few minutes\n!pip install --upgrade azureml-sdk[webservice-schema]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\n\nprint(\"This notebook was created using version 1.0.17 of the Azure ML SDK\")\nprint(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Configure your Azure ML workspace\n\n### Workspace parameters\n\nTo use an AML Workspace, you will need to import the Azure ML SDK and supply the following information:\n* Your subscription id\n* A resource group name\n* (optional) The region that will host your workspace\n* A name for your workspace\n\nYou can get your subscription ID from the [Azure portal](https://portal.azure.com).\n\nYou will also need access to a [_resource group_](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview#resource-groups), which organizes Azure resources and provides a default region for the resources in a group.  You can see what resource groups to which you have access, or create a new one in the [Azure portal](https://portal.azure.com).  If you don't have a resource group, the create workspace command will create one for you using the name you provide.\n\nThe region to host your workspace will be used if you are creating a new workspace.  You do not need to specify this if you are using an existing workspace. You can find the list of supported regions [here](https://azure.microsoft.com/en-us/global-infrastructure/services/?products=machine-learning-service).  You should pick a region that is close to your location or that contains your data.\n\nThe name for your workspace is unique within the subscription and should be descriptive enough to discern among other AML Workspaces.  The subscription may be used only by you, or it may be used by your department or your entire enterprise, so choose a name that makes sense for your situation.\n\nThe following cell allows you to specify your workspace parameters.  This cell uses the python method `os.getenv` to read values from environment variables which is useful for automation.  If no environment variable exists, the parameters will be set to the specified default values.  \n\nIf you ran the Azure Machine Learning [quickstart](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started) in Azure Notebooks, you already have a configured workspace!  You can go to your Azure Machine Learning Getting Started library, view *config.json* file, and copy-paste the values for subscription ID, resource group and workspace name below.\n\nReplace the default values in the cell below with your workspace parameters"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\nsubscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<my-subscription-id>\")\nprint(subscription_id)\nresource_group = os.getenv(\"RESOURCE_GROUP\", default=\"my-automl-workshop-rg\")\nprint(resource_group)\nworkspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"my-automl-workshop-ws\")\nprint(workspace_name)\nworkspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"eastus2\")\nprint(workspace_region)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Access your workspace\n\nThe following cell uses the Azure ML SDK to attempt to load the workspace specified by your parameters.  If this cell succeeds, your notebook library will be configured to access the workspace from all notebooks using the `Workspace.from_config()` method.  The cell can fail if the specified workspace doesn't exist or you don't have permissions to access it. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\ntry:\n    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n    # write the details of the workspace to a configuration file to the notebook library\n    ws.write_config()\n    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\nexcept:\n    print(\"Workspace not accessible. Change your parameters or create a new workspace below\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a new workspace\n\nIf you don't have an existing workspace and are the owner of the subscription or resource group, you can create a new workspace.  If you don't have a resource group, the create workspace command will create one for you using the name you provide.\n\n**Note**: As with other Azure services, there are limits on certain resources (for example AmlCompute quota) associated with the Azure ML service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota.\n\nThis cell will create an Azure ML workspace for you in a subscription provided you have the correct permissions.\n\nThis will fail if:\n* You do not have permission to create a workspace in the resource group\n* You do not have permission to create a resource group if it's non-existing.\n* You are not a subscription owner or contributor and no Azure ML workspaces have ever been created in this subscription\n\nIf workspace creation fails, please work with your IT admin to provide you with the appropriate permissions or to provision the required resources."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\n# Create the workspace using the specified parameters\nws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      create_resource_group = True,\n                      exist_ok = True)\nws.get_details()\n\n# write the details of the workspace to a configuration file to the notebook library\nws.write_config()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Experiment Setup\nImport packages required for the experiment"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import libraries\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport logging\nimport warnings\n# Squash warning messages for cleaner output in the notebook\nwarnings.showwarning = lambda *args, **kwargs: None\n\n\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.run import AutoMLRun\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Verify the config file\nWe read the config.json file and verify the experiment workspace, subscription etc. are defined correctly."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Retrieve workspace\nws = Workspace.from_config()\n\n# choose a name for the run history container in the workspace\nexperiment_name = 'automl-energydemandforecasting'\n# project folder\nproject_folder = './sample_projects/automl-local-energydemandforecasting'\n\nexperiment = Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Run History Name'] = experiment_name\npd.set_option('display.max_colwidth', -1)\noutputDf = pd.DataFrame(data = output, index = [''])\noutputDf.T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Data\nRead energy demanding data from file, and preview data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pd.read_csv(\"nyc_energy.csv\", parse_dates=['timeStamp'])\ndata.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "display(data.describe().T)\ndata.demand.hist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Split the data\nWe are setting aside data that is newer than Feb 1st 2017 as test data set. All the data that is older that Feb 1st 2017 is the training data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = data[data['timeStamp'] < '2017-02-01']\ntest = data[data['timeStamp'] >= '2017-02-01']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Prepare the test data, we will feed X_test to the fitted model and get prediction"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_test = test.pop('demand').values\nX_test = test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Split the train data to train and valid\nUse one month's data as valid data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train = train[train['timeStamp'] < '2017-01-01']\nX_valid = train[train['timeStamp'] >= '2017-01-01']\ny_train = X_train.pop('demand').values\ny_valid = X_valid.pop('demand').values\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_valid.shape)\nprint(y_valid.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Train Using Automated ML\nYou will configure automated ML and run automated ML experiment which will generate Machine Learning Models. The training jobs are run on VMs provided and managed by Azure Notebooks. \nThere are many configuration parameters you can define for your experiment. . Read more in the [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Configure Automated ML Experiment"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Set the time series column\ntime_column_name = 'timeStamp'\nautoml_settings = {\n    \"time_column_name\": time_column_name,\n}\n\n# create the configuration object\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_nyc_energy_errors.log',\n                             primary_metric='normalized_root_mean_squared_error',\n                             iterations = 5,\n                             iteration_timeout_minutes = 10,\n                             X = X_train,\n                             y = y_train,\n                             X_valid = X_valid,\n                             y_valid = y_valid,\n                             path=project_folder,               \n                             blacklist_models = ['RandomForest'],\n                             verbosity = logging.INFO,\n                            **automl_settings)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can call the submit method on the experiment object and pass the run configuration. For Local runs the execution is synchronous. Depending on the data and number of iterations this can run for while.\nYou will see the currently running iterations printing to the console."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run = experiment.submit(automl_config, show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# View the run summary\nlocal_run",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## Widget for monitoring runs"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(local_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Retrieve All Child Runs"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "children = list(local_run.get_children())\nmetricslist = {}\nfor run in children:\n    properties = run.get_properties()\n    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}\n    metricslist[int(properties['iteration'])] = metrics\n\nrundata = pd.DataFrame(metricslist).sort_index(1)\nrundata",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Retrieve the Best Model\nBelow we select the best pipeline from our iterations. The get_output method returns the best run and the fitted model. The Model includes the pipeline and any pre-processing. Overloads on get_output allow you to retrieve the best run and fitted model for any logged metric or for a particular iteration."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = local_run.get_output()\nprint(best_run)\nprint(fitted_model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Best Model Based on Any Other Metric\nShow the run and the model that has the smallest log_loss value"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lookup_metric = \"r2_score\"\nbest_run, fitted_model = local_run.get_output(metric = lookup_metric)\nprint(best_run)\nprint(fitted_model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Model from a Specific Iteration\nShow the run and the model from the third iteration:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "iteration = 3\nthird_run, third_model = local_run.get_output(iteration = iteration)\nprint(third_run)\nprint(third_model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Continue experiment for more iterations (Optional)\nIf you want you can continue to run more iterations,picking up where you left off. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# continue training for another couple of iterations\nlocal_run.continue_experiment(X=X_train, y=y_train,X_valid = X_valid,y_valid = y_valid,iterations = 2)\nfrom azureml.widgets import RunDetails\nRunDetails(local_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Retrieve explanation for best model\nModel explainability is important to understand the features and their importance. This will retrieve the explainability of the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Commented out, please run after Workshop as takes some time\n\n#from azureml.train.automl.automlexplainer import explain_model\n#shap_values, expected_values, overall_summary, overall_imp, per_class_summary, per_class_imp = \\\n#   explain_model(fitted_model, X_train, X_test, best_run)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Test the Best Fitted Model\nPredict on training and test set, and calculate residual values."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred = fitted_model.predict(X_test)\ny_pred",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Use the Check Data Function to remove the nan values from y_test to avoid error when calculate metrics"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "if len(y_test) != len(y_pred):\n    raise ValueError(\n        'the true values and prediction values do not have equal length.')\nelif len(y_test) == 0:\n    raise ValueError(\n        'y_true and y_pred are empty.')\n\n# if there is any non-numeric element in the y_true or y_pred,\n# the ValueError exception will be thrown.\ny_test_f = np.array(y_test).astype(float)\ny_pred_f = np.array(y_pred).astype(float)\n\n# remove entries both in y_true and y_pred where at least\n# one element in y_true or y_pred is missing\ny_test = y_test_f[~(np.isnan(y_test_f) | np.isnan(y_pred_f))]\ny_pred = y_pred_f[~(np.isnan(y_test_f) | np.isnan(y_pred_f))]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Plot the predictions to compare to actual data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"[Test Data] \\nRoot Mean squared error: %.2f\" % np.sqrt(mean_squared_error(y_test, y_pred)))\n# Explained variance score: 1 is perfect prediction\nprint('mean_absolute_error score: %.2f' % mean_absolute_error(y_test, y_pred))\nprint('R2 score: %.2f' % r2_score(y_test, y_pred))\n\n# Plot outputs\ntest_pred = plt.scatter(y_test, y_pred, color='b')\ntest_test = plt.scatter(y_test, y_test, color='g')\nplt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Deploy\nDeploy the model into an Azure Container Instance to enable inferencing on new data\n\n## Register the model\nRegister the best model to the AML service"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = local_run.register_model(description = 'automated ml model for energy demand forecasting', tags = {'ml': \"Forecasting\", 'type': \"automl\"})\nprint(local_run.model_id) # This will be written to the script file later in the notebook.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create Scoring Script\nThis will be used to run the model on new data for predictions"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile score_energy_demand.py\nimport pickle\nimport json\nimport numpy as np\nimport azureml.train.automl\nfrom sklearn.externals import joblib\nfrom azureml.core.model import Model\n\n\ndef init():\n    global model\n    model_path = Model.get_model_path(model_name = '<<modelid>>') # this name is model.id of model that we want to deploy\n    # deserialize the model file back into a sklearn model\n    model = joblib.load(model_path)\n\ndef run(timestamp,precip,temp):\n    try:\n        rawdata = json.dumps({timestamp, precip, temp})\n        data = json.loads(rawdata)\n        data_arr = numpy.array(data)\n        result = model.predict(data_arr)\n        # result = json.dumps({'timeStamp':timestamp, 'precip':precip, 'temp':temp})\n    except Exception as e:\n        result = str(e)\n        return json.dumps({\"error\": result})\n    return json.dumps({\"result\":result.tolist()})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a YAML File for the EnvironmentÂ¶ \nThe YAML file will be used to setup the conda environment on the deployed image"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Retrieve the dependencies\nexperiment = Experiment(ws, experiment_name)\nml_run = AutoMLRun(experiment = experiment, run_id = local_run.id)\ndependencies = ml_run.get_run_sdk_dependencies(iteration = 0)\nfor p in ['azureml-train-automl', 'azureml-sdk', 'azureml-core']:\n    print('{}\\t{}'.format(p, dependencies[p]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the environment file\n\nfrom azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn'], pip_packages=[\"azureml-train-automl\"])\nprint(myenv.serialize_to_string())\n\nconda_env_file_name = 'my_conda_env.yml'\nmyenv.save_to_file('.', conda_env_file_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Substitute the actual version number in the environment file.\n# This is not strictly needed in this notebook because the model should have been generated using the current SDK version.\n# However, we include this in case this code is used on an experiment from a previous SDK version.\n\nwith open(conda_env_file_name, 'r') as cefr:\n    content = cefr.read()\n\nwith open(conda_env_file_name, 'w') as cefw:\n    cefw.write(content.replace(azureml.core.VERSION, dependencies['azureml-sdk']))\n\n# Substitute the actual model id in the script file.\n\nscript_file_name = 'score_energy_demand.py'\n\nwith open(script_file_name, 'r') as cefr:\n    content = cefr.read()\n\nwith open(script_file_name, 'w') as cefw:\n    cefw.write(content.replace('<<modelid>>', local_run.model_id))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Generate schema file\nSchema file is used to define the deployed web service REST API, so it is consumable from \"Swagger enabled\" services, such as Power BI"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.webservice_schema.sample_definition import SampleDefinition\nfrom azureml.webservice_schema.data_types import DataTypes\nfrom azureml.webservice_schema.schema_generation import generate_schema\n\nschema_file_name = './schema.json'\ndef run(timestamp,precip,temp):\n    return \"OK\"\n\nimport numpy as np\ngenerate_schema(run, inputs={\n    \"timestamp\" : SampleDefinition(DataTypes.STANDARD, '2012-01-01 00:00:00'),\n    \"precip\" : SampleDefinition(DataTypes.STANDARD, '0.0'),\n    \"temp\" : SampleDefinition(DataTypes.STANDARD, '0.0')}, \n    filepath=schema_file_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a Docker file to include extra dependencies in the image\nThe container image will be based on the model and is used to deploy the container instance"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile docker_steps.dockerfile\nRUN apt-get update && \\\n    apt-get upgrade -y && \\\n    apt-get install -y build-essential gcc g++ python-dev unixodbc unixodbc-dev",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "docker_file_name = \"docker_steps.dockerfile\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a Container Image\nThe container image will be based on the model and is used to deploy the container instance"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.image import Image, ContainerImage\n\nimage_config = ContainerImage.image_configuration(runtime= \"python\",\n                                 execution_script = script_file_name,\n                                 docker_file = docker_file_name,\n                                 schema_file = schema_file_name,\n                                 conda_file = conda_env_file_name,\n                                 tags = {'ml': \"Forecasting\", 'type': \"automl\"},\n                                 description = \"Image for automated ml energy demand forecasting predictions\")\n\nimage = Image.create(name = \"automlenergyforecasting\",\n                     models = [model],\n                     image_config = image_config, \n                     workspace = ws)\n\nimage.wait_for_creation(show_output = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Deploy the Image as a Web Service on Azure Container Instance"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n                                               memory_gb = 1, \n                                               tags = {'ml': \"Forecasting\", 'type': \"automl\"}, \n                                               description = 'ACI service for automated ml energy demand forecasting predictions')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import Webservice\n\naci_service_name = 'automlenergyforecasting1'\nprint(aci_service_name)\naci_service = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                           image = image,\n                                           name = aci_service_name,\n                                           workspace = ws)\naci_service.wait_for_deployment(True)\nprint(aci_service.state)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Recap\n\nIn this workshop we have successfully\n* Imported a CSV file for training \n* Split the data for test and validation\n* Configured an automated ML experiment\n* Run an automated ML experiment\n* Reviewed the results\n* Taken the best best model and plotted predications\n* Deployed a model as a web service ready to make new inferences"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# Resources\nAutomated ML & Azure Machine Learning Resources\n\n* AskAutomatedML@microsoft.com\n* https://aka.ms/AutomatedML\n* https://aka.ms/AutomatedMLDocs\n* https://github.com/Azure/MachineLearningNotebooks"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}